{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d88c1966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import *\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3244b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('faqs.json', encoding=\"utf-8\").read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d955bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        # take each word and tokenize it\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        # adding documents\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # adding classes to our class list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae051b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 documents\n",
      "64 classes ['Amyotrophic Lateral Sclerosis', 'Are COVID-19 vaccines safe for people living with HIV', 'Are antibiotics effective in preventing or treating the COVID-19', 'Are people with tuberculosis likely to be at increased risk of COVID-19 infection, illness and death', 'Are pools and water areas safe to use', 'Are there any medicines or therapies that can prevent or cure COVID-19', 'Are there precautions to take while eating in a restaurant', 'As a smoker, am I likely to get more severe symptoms if infected', 'As a smoker, is my risk of getting the COVID-19 virus higher than that of a non-smoker', 'Asthma', 'COVID-19 (Coronavirus) & COPD', 'COVID-19 (Coronavirus) & Cancer', 'COVID-19 (Coronavirus) & Cystic Fibrosis ', 'COVID-19 (Coronavirus) & Diabetes', 'Can I have the second dose with a different vaccine than the first dose', 'Can we stop taking precautions after being vaccinated', 'Can you get the virus from people who were in the room previous to you', 'Dementia', 'Do COVID-19 and tuberculosis spread in the same way', 'Do COVID-19 vaccines provide protection for people living with HIV', 'Do the vaccines protect against variants', 'Does nicotine use affect my chances in the context of COVID-19', 'How are COVID-19 and influenza viruses different', 'How are COVID-19 and influenza viruses similar', 'How can ventilation reduce the risk of contracting COVID-19 in airplanes', 'How do we know that COVID-19 vaccines are safe', 'How does COVID 19 spread?', 'How does COVID-19 affect people with RA', 'How is the research and development process being accelerated without compromising safety', 'How likely am I to catch COVID-19?', 'How quickly could COVID-19 vaccines stop the pandemic', 'How should people with Asthma wear masks.', 'Is it possible that someone vaccinated against COVID-19 will still get infected', 'Is it safe for pregnant women, those planning to become pregnant, and breastfeeding mothers to receive COVID-19 vaccines', 'Is the vaccine safe for children', 'Is there a vaccine for COVID-19', 'Recommendation for Amyotrophic Lateral Sclerosis', 'Routine Care for Cystic Fibrosis during COVID-19', 'Should I be vaccinated if I have had COVID-19', 'Should I worry about COVID-19', 'Should women who are on their periods take the COVID-19 vaccine', 'So how do I stay safe while exercising in COVID-19', 'Under what circumstances should a COVID-19 vaccine be recalled', 'What are the benefits of getting vaccinated', 'What are the side effects of COVID-19 vaccines', 'What are the symptoms of COVID 19?', 'What can I do to protect myself and prevent the spread of disease?', 'What extra precautions should be taken by people with Asthma?', 'What is COVID-19?', 'What is a coronavirus?', 'What is the link between COVID-19 vaccines and allergic reactions', 'What is ventilation and how can it prevent COVID-19 from spreading', 'What medical interventions are available for COVID-19 and influenza viruses?', 'What precautions should I take during travel', 'What precautions should everyone take in a hotel or other accommodation establishment', 'What should I do if I get sick while traveling', 'Who should be excluded from receiving COVID-19 vaccines', 'Who should get the COVID-19 vaccines', 'Who should not travel', 'Will other vaccines help protect me from COVID-19', 'changing', 'goodbye', 'greeting', 'virus mutates']\n",
      "221 unique lemmatized words ['&', \"'s\", '(', ')', ',', '.', '19', 'a', 'about', 'accelerated', 'accommodation', 'affect', 'after', 'against', 'airplane', 'allergic', 'am', 'amyotrophic', 'and', 'antibiotic', 'any', 'anyone', 'are', 'area', 'asthma', 'at', 'available', 'be', 'become', 'being', 'benefit', 'between', 'breastfeeding', 'by', 'bye', 'can', 'cancer', 'care', 'catch', 'chance', 'change', 'changing', 'child', 'circumstance', 'compromising', 'concerned', 'context', 'contracting', 'copd', 'coronavirus', 'could', 'covid', 'covid-19', 'cure', 'cya', 'cystic', 'day', 'death', 'dementia', 'development', 'diabetes', 'different', 'disease', 'do', 'doe', 'dose', 'during', 'eating', 'effect', 'effected', 'effective', 'establishment', 'everyone', 'excluded', 'exercising', 'extra', 'fibrosis', 'first', 'for', 'from', 'get', 'getting', 'good', 'goodbye', 'had', 'have', 'hello', 'help', 'hi', 'higher', 'hiv', 'hotel', 'how', 'i', 'if', 'illness', 'impact', 'in', 'increased', 'infected', 'infection', 'influenza', 'intervention', 'is', 'it', 'know', 'later', 'lateral', 'leaving', 'likely', 'link', 'living', 'mask', 'me', 'mean', 'measure', 'medical', 'medicine', 'meet', 'more', 'mother', 'mutates', 'my', 'myself', 'nice', 'nicotine', 'non-smoker', 'not', 'of', 'on', 'or', 'other', 'pandemic', 'patient', 'people', 'period', 'planning', 'pool', 'possible', 'precaution', 'pregnant', 'prevent', 'preventing', 'previous', 'process', 'protect', 'protection', 'provide', 'quickly', 'ra', 'reaction', 'recalled', 'receive', 'receiving', 'recommendation', 'reduce', 'research', 'restaurant', 'risk', 'room', 'routine', 'safe', 'safety', 'same', 'sars-cov-2', 'say', 'sclerosis', 'second', 'see', 'severe', 'should', 'sick', 'side', 'similar', 'smoker', 'so', 'someone', 'spread', 'spreading', 'stay', 'still', 'stop', 'symptom', 'take', 'taken', 'taking', 'than', 'that', 'the', 'their', 'therapy', 'there', 'those', 'to', 'travel', 'traveling', 'treating', 'tuberculosis', 'under', 'up', 'use', 'vaccinated', 'vaccine', 'variant', 'ventilation', 'virus', 'water', 'way', 'we', 'wear', 'were', 'what', 'whats', 'while', 'who', 'will', 'with', 'without', 'woman', 'worry', 'you']\n"
     ]
    }
   ],
   "source": [
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8de82dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-3544dc409e53>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "    # initializing bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae0138fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 2s 1ms/step - loss: 0.0154\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0135\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0126\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0123\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0123\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0123\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0113\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0110\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0093\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0091\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0097\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0087\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0095\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0096\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.0102\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0090\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0101\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0090\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0084\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0096\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0078\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.0072\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0090\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0071\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0079\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0072\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0063\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0077\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#fitting and saving the model\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "659f7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('chatbot_model.h5')\n",
    "import json\n",
    "import random\n",
    "intents = json.loads(open('faqs.json', encoding=\"utf-8\").read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e26b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859a93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GUI with tkinter\n",
    "import tkinter\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "\n",
    "    if msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
    "\n",
    "        res = chatbot_response(msg)\n",
    "        ChatLog.insert(END, \"Bot: \" + res + '\\n\\n')\n",
    "\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "\n",
    "\n",
    "base = Tk()\n",
    "base.title(\"Hello\")\n",
    "base.geometry(\"400x500\")\n",
    "base.resizable(width=FALSE, height=FALSE)\n",
    "\n",
    "#Create Chat window\n",
    "ChatLog = Text(base, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
    "\n",
    "ChatLog.config(state=DISABLED)\n",
    "\n",
    "#Bind scrollbar to Chat window\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "#Create Button to send message\n",
    "SendButton = Button(base, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "                    bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
    "                    command= send )\n",
    "\n",
    "#Create the box to enter message\n",
    "EntryBox = Text(base, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
    "#EntryBox.bind(\"<Return>\", send)\n",
    "\n",
    "\n",
    "#Place all components on the screen\n",
    "scrollbar.place(x=376,y=6, height=386)\n",
    "ChatLog.place(x=6,y=6, height=386, width=370)\n",
    "EntryBox.place(x=128, y=401, height=90, width=265)\n",
    "SendButton.place(x=6, y=401, height=90)\n",
    "\n",
    "base.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d224c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb92dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
